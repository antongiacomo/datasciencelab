---
title: "Learning in Adversarial Settings: Breaking Models by Changing World View"
description: ''
type: Young

tags: []
date: 07-11-2025
time: 11:00
people:
    -   name: Fabio De Gaspari
        affiliation: Sapienza University of Rome
        bio: Fabio De Gaspari is an Assistant Professor at Sapienza University of Rome, Italy. Previously, he was a Research Associate at the Sapienza University of Rome, in 2019 and 2020. He obtained his Ph.D. Computer Science at the Sapienza University of Rome, Italy, in 2019 under the supervision of Prof. Luigi Vincenzo Mancini and co-supervision of Prof. Mauro Conti. He was a visiting researcher at the Center for Secure Information Systems, George Mason University, USA, in 2016, at Cisco Systems, Issy-les-Moulineaux, France, in 2017, and at the Oxford Systems Security Lab, Oxford University, UK, in 2020. His research focuses on systems security, machine learning security, and network security. He has published numerous papers in high-profile conferences and journals such as IEEE TIFS, IEEE TDSC, IEEE S&P, ESORICS, IEEE/IFIP DSN, and ACM AsiaCCS. He is an active member of the cybersecurity research community, participating in the organization, program committees, and editorial staff of renowned conferences and journals such as IEEE S&P, USENIX, ESORICS, IEEE TIFS, and Computers & Security.
        homepage: https://sites.google.com/di.uniroma1.it/degaspari/home

location: https://teams.microsoft.com/l/meetup-join/19%3aS2RBj6wmgm2Zjk3jx07ydAsihsKI8KSIkkQRSStaP7E1%40thread.tacv2/1761213369351?context=%7b%22Tid%22%3a%2213b55eef-7018-4674-a3d7-cc0db06d545c%22%2c%22Oid%22%3a%223b92e2cc-3616-4070-82ad-a9f97e1e92ac%22%7d
---

### Abstract

Recent advancements in machine learning are driving us toward a more automated cyberspace. Modern machine learning models achieve super-human performance across diverse tasks, and the emergence of large language models promises even more rapid advancements in the near future. However, there is more to the story than just performance. In this talk, we'll take a closer look at today's machine learning models from a security angle, focusing on how they perform in adversarial settings where adversaries may try to exploit their weaknesses. Can we really trust these models to make fair decisions in critical areas such as cyber defense? Can we be sure that models are safe to deploy on sensitive systems? Can we publicly and safely share models with others without worry or risk?